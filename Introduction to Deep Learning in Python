#from datacamp
# The data point you will make a prediction for
input_data = np.array([0, 3])

# Sample weights
weights_0 = {'node_0': [2, 1],
             'node_1': [1, 2],
             'output': [1, 1]
            }

# The actual target value, used to calculate the error
target_actual = 3

# Make prediction using original weights
model_output_0 = predict_with_network(input_data, weights_0)

# Calculate error: error_0
error_0 = model_output_0 - target_actual

# Create weights that cause the network to make perfect prediction (3): weights_1
weights_1 = {'node_0': [2, 1],
             'node_1': [1, 2],
             'output': [1, 0]
            }

# Make prediction using new weights: model_output_1
model_output_1 = predict_with_network(input_data, weights_1)

# Calculate error: error_1
error_1 = model_output_1 - target_actual

# Print error_0 and error_1
print(error_0)
print(error_1)



# back propagation
# Set the learning rate: learning_rate
learning_rate = 0.01

# Calculate the predictions: preds
preds = (weights * input_data).sum()

# Calculate the error: error
error = preds - target

# Calculate the slope: slope
slope = 2 * input_data * error

# Update the weights: weights_updated
weights_updated =weights-learning_rate*slope

# Get updated predictions: preds_updated
preds_updated = (weights_updated*input_data).sum()

# Calculate updated error: error_updated
error_updated =preds_updated-target

# Print the original error
print(error)

# Print the updated error
print(error_updated)
#making multiple update to weights
n_updates = 20
mse_hist = []

# Iterate over the number of updates
for i in range(n_updates):
    # Calculate the slope: slope
    slope = get_slope(input_data, target, weights)
    
    # Update the weights: weights
    weights = weights - 0.01 * slope
    
    # Calculate mse with new weights: mse
    mse = get_mse(input_data, target, weights)
    
    # Append the mse to mse_hist
    mse_hist.append(mse)

# Plot the mse history
plt.plot(mse_hist)
plt.xlabel('Iterations')
plt.ylabel('Mean Squared Error')
plt.show()


# Import necessary modules
import keras
from keras.layers import Dense
from keras.models import Sequential

# Save the number of columns in predictors: n_cols
n_cols = predictors.shape[1]

# Set up the model: model
model =Sequential()

# Add the first layer
model.add(Dense(50,activation='relu',input_shape=(n_cols,)))

# Add the second layer
model.add(Dense(32,activation='relu',input_shape=(n_cols,)))
# Add the output layer
model.add(Dense(1))

# compiling and fitting 

# Import necessary modules
import keras
from keras.layers import Dense
from keras.models import Sequential

# Specify the model
n_cols = predictors.shape[1]
model = Sequential()
model.add(Dense(50, activation='relu', input_shape = (n_cols,)))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))

# Compile the model
model.compile(optimizer='adam',loss='mean_squared_error')

# Verify that model contains information from compiling
print("Loss function: " + model.loss)

model.compile(optimizer='adam', loss='mean_squared_error')

# Fit the model
model.fit(predictors,target)


# Import necessary modules
import keras
from keras.layers import Dense
from keras.models import Sequential
from keras.utils import to_categorical

# Convert the target to categorical: target
target = to_categorical(df.survived)

# Set up the model
model=Sequential()

# Add the first layer
model.add(Dense(32,activation='relu',input_shape=(n_cols,)))

# Add the output layer
model.add(Dense(2,activation='softmax'))

# Compile the model
model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])
# Fit the model
model.fit(predictors,target)

# predict data 
# Specify, compile, and fit the model
model = Sequential()
model.add(Dense(32, activation='relu', input_shape = (n_cols,)))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='sgd', 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])
model.fit(predictors, target)

# Calculate predictions: predictions
predictions =model.predict(pred_data)

# Calculate predicted probability of survival: predicted_prob_true
predicted_prob_true =predictions[:,1]
# print predicted_prob_true
print(predicted_prob_true)


BAckpropagation:
Start at some random set of weights
Use forward propagation to make a prediction
Use backward propagation to calculate the slope of the loss function w.r.t each weight

Saving reloading and use your model

from keras.models import load_model
model.save('model_file.h5‘）
my_model=load_model('my_model.h5’)
predictions=my_model.predict(data_to_predict_with)
probability_true= predictions[:,1]


#Stochastic gradient descent

def get_new_model(input_shape=input_shape):
	model= Sequential()
	model.add(Dense(100,activation='relu',input_shape=input_shape)
	model.add(Dense(100,activation='relu))
	model.add(Dense(2,activation='softmax')
	return(model)

lr_to_test=[.000001,0.01,1]

# loop over learning rates
for lr in lr_to_test:
	model=get_new_model()
	my_optimizer=SGD(lr=lr)
	model.compile(optimizer=my_optimizer,loss='categorical_crossentropy')
	model.fit(predictors,target)

# model validation
model.fit(predictors,target,validation_split=0.3)
#Early stopping
from keras.callbacks import EarlyStopping
early_stopping_monitor=EarlyStopping(patience=2) # 2 or 3 are reasonable 
#if you see 3 3pochs with no improvement, it's unlikely to turn around and start improving agian.

# callbacks is a list 
# nb_epoch is maxmum epochs number
# keras will go the number of epochs validation loss stops improving, in which case it will stop earlier. 
# Import EarlyStopping
from keras.callbacks import EarlyStopping

# Save the number of columns in predictors: n_cols
n_cols = predictors.shape[1]
input_shape = (n_cols,)

# Specify the model
model = Sequential()
model.add(Dense(100, activation='relu', input_shape = input_shape))
model.add(Dense(100, activation='relu'))
model.add(Dense(2, activation='softmax'))

# Compile the model
model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
# Define early_stopping_monitor
early_stopping_monitor = EarlyStopping(patience=2)

# Fit the model
model.fit(predictors,target,epochs=30,validation_split=0.3,callbacks=[early_stopping_monitor])


# compare two models
# Define early_stopping_monitor
early_stopping_monitor = EarlyStopping(patience=2)

# Create the new model: model_2
model_2 = Sequential()

# Add the first and second layers
model_2.add(Dense(100,activation='relu',input_shape=input_shape))
model_2.add(Dense(100,activation='relu'))

# Add the output layer
model_2.add(Dense(2,activation='softmax'))

# Compile model_2
model_2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
# Fit model_1
model_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)

# Fit model_2
model_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)

# Create the plot
plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')
plt.xlabel('Epochs')
plt.ylabel('Validation score')
plt.show()
